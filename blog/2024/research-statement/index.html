<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Research Statement | Digital Fenestra </title> <meta name="author" content="Krishn V. Kher"> <meta name="description" content="An exhaustive list of research directions of particular interest"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/Web.png?c6eb16720d27627cd1415157bb446091"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://krishnkher.github.io/blog/2024/research-statement/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Digital Fenestra </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">Blog</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Research Statement</h1> <p class="post-meta"> Created in June 22, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/research-statement"> <i class="fa-solid fa-hashtag fa-sm"></i> research-statement,</a>   <a href="/blog/tag/research-proposal"> <i class="fa-solid fa-hashtag fa-sm"></i> research-proposal</a>   ·   <a href="/blog/category/official"> <i class="fa-solid fa-tag fa-sm"></i> official</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This post shows how to add bibliography to simple blog posts. We support every citation style that <a href="https://github.com/inukshuk/jekyll-scholar" rel="external nofollow noopener" target="_blank">jekyll-scholar</a> does. That means simple citation like (missing reference), multiple citations like (missing reference), long references like (missing reference) or also quotes:</p> <blockquote> <p>Lorem ipsum dolor sit amet, consectetur adipisicing elit,<br>sed do eiusmod tempor.</p> <p>Lorem ipsum dolor sit amet, consectetur adipisicing.</p> <cite>(missing reference)</cite> </blockquote> <p>If you would like something more academic, check the <a href="/blog/2021/distill/">distill style post</a>.</p> <h1 id="on-the-role-of-computation-logic--memory-in-learning">On the Role of Computation, Logic, &amp; Memory in Learning</h1> <h2 id="prelude-labelsecpre">Prelude \label{sec:pre}</h2> <p>“If it sees like a human, converses like a human, and locomotes like a human, then it could very well \emph{not} be a human!” This arguably summarizes the performance of modern day machine learning models and algorithms on so-called System-I systems \cite{10.5555/3295222.3295288, goyal2022inductive}, such as deep CNNs \cite{10.5555/2999134.2999257, 7298594, 7780459}, N-gram models \cite{brown-etal-1992-class, bird-loper-2004-nltk, 10.5555/2999792.2999959}, Hidden Markov Models \cite{8187420, pmlr-v2-gruber07a} (e.g. for speech recognition \cite{graves2013speech}), etc. In certain tasks, such machines appear to even outperform human capabilities, such as in arithmetic / logical computations \cite{6448961, 10.5555/3327757.3327899}, raw memory storage and retrieval, etc. However, even given the slightly more realistic goal of only mimicking (and not duplicating!) cognitive abilities of humans, we are understandably far off on System-II tasks that involve reasoning, planning, long term decision making, etc. These cannot be merely achieved by scaling up resources in systems that purely compute System-I tasks. In this connection, we suggest Figure \ref{fig:1} as an abstract, simple, and helpful mental picture of task allocation within the human cognitive system, and especially as a roadmap for different research directions we envisage in Section \ref{sec:RA}, based on work in the literature. We refer to this as the “Personalized Cognitive Assistant” model (or PCA for short) hereafter.</p> <p>\begin{figure}[h] \centering \includegraphics[width=0.75\textwidth]{PCA.jpg} \caption{\textcolor{blue}{A comprehensive view of the \textcolor{OliveGreen}{\textbf{PCA}} model}} \end{figure}\label{fig:1}</p> <p>Notice the emphasis on different aspects of the cognitive system in \ref{fig:1}. We briefly describe the intended mechanisms and workings of the components here, and discuss more details later. The energy transducers, (I) and (O) map to the stimuli received from external environments in form of multimodal input and the corresponding outputs emitted by the system based on user intent (represented by the user icon), again in multimodal fashion. The term \emph{energy transducer} (ET) is significant as it conveys the idea that though stimuli are received in different forms of energy (visual electromagnetic waves, mechanical auditory waves, direct contact, etc.), these are usually converted into electromagnetic forms of energy of uniform nature (in a limited range of frequencies, to enable neural processing). \par The ETs only interact with the \emph{sea of impressions} (SI). All the data that is retained within the system is essentially stored in some abstract representation, possibly as another form of energy in the SI. The SI needs resources to physically store allusions to the data it has received and processed, which is where it uses neurons (analogous to neural networks). This also performs the important function of suggesting intent to the user based on the collection of impressions it continually receives and embeds. The oval corresponds to components that require this physical resource of \textbf{\emph{memory}}. \par</p> <p>Next, the SI interacts with the \emph{rule synthesizer} module, that belongs to the rectangle, representing the central intelligence unit (CIU). The CIU is what primarily interacts with user intent. The \emph{rule synthesizer} unit acts to generate patterns of data based on the data collected and further aims to continually refactor the memories encoded into the sea of impressions, autoregressively. This is then aided with a \emph{rule evaluator} unit, such as the parts of the brains that perform operations like arithmetic/logical computations, search, retrieval, etc. Furthermore, we have the \emph{autonoetic evaluator unit} unit, whichthat tries to perform long-term decision- making and counterfactual simulations based on the sea of impressions and the rules synthesized in conjunction with user intent by the \emph{rule synthesis} unit. Thus, theseis modules incorporates the \emph{rule evaluator} unit to perform \emph{computations} that aids it in decision- making. Thus the rule-based units are where the maximum \textbf{\emph{logic}}-based operations and \textbf{\emph{computation}} occur. Lastly, the \emph{emotional interpreter} unit seeks to incorporate user emotions and make judgements based on such emotions. This is the closest to user intent. Unfortunately, we are largely unaware of modelling strategies for this unit and thus refrain from focusing on this aspect of the system for now, except for using it as a proxy for stimulating artificial creativity and generation in the model. The other \emph{PCA} engines depicted in Figure \ref{fig:1} is to emphasize how \emph{communication} is an important resource for these systems to learn and interact withamongst each other. Even within a single PCA engine, communication between modules is essential, as is evident from Figure \ref{fig:1}. The lines in pairs bear the convention that the green line depicts the direction of command flow from one module to the other – the module away from the arrow head commands an (appropriate) instruction to the module adjacent to the arrow head, while the result of the instruction is reciprocated to the commanding module. With the exception of the \emph{ET} units, the uni-directional arrows depict usage of services of the module closer to the arrow-head by the module away from the arrow head. In case of the \emph{ET} units, it solely indicates the direction of input/output flow. \par</p> <p>The broad goal of this \emph{research proposal} is to implement PCA, end-2-end, in a scalable and resource-efficient manner. The specific focus is on the memory units, rule-based units, and on generative modelling, wherein we wish to adopt a compression- based approach to learning. In short, we would like to gradationally differentiate the skills required to solve the following tasks: \begin{enumerate} \item \textcolor{blue}{Memorize the data}. \item \textcolor{blue}{Infer properties or rules that govern generation of the data that help answer queries pertaining to the data}. \item \textcolor{blue}{Generate data}. \end{enumerate} Intuitively, solving task $i$ would entail solving task $i-1$ ($i&gt;1$). It is of great wonder as to how the human brain with its set of a few billion neurons and minimal power consumption does all these tasks! We delineate possible ventures to research each of these aspects, to build an autarkic, bespoke system that aids users in personal analysis and communications with other users. Before we move on to the next section, we would like to emphasize how the AI that we envision induces a positive bias towards responsibility in that no component of the system aims to overpower or steer task execution beyond conscious user intent input to the system. Thus, we do not focus on or intend to automate tasks that explicitly involve freewill. Our automation solely concerns components that require computational resources for computing quantities governed by perceived environment of a user and logical rules that comply with the user’s intent. This ties in line with research and societal values that advocate for the democratization of responsible AI for social good.</p> <h2 id="research-agendra-labelsecra">Research Agendra \label{sec:RA}</h2> <h3 id="primary-labelsubsecprimary">Primary \label{subsec:primary}</h3> <p>In this section, we elaborate on the modules presented in Figure \ref{fig:1} in a roughly bottom-up manner, as we describe the salient questions that deserve dedicated research efforts. \begin{itemize} \item \textbf{Energy Transducers}: As far as receiving and emitting sensory input/output is concerned, we primarily concern ourselves with downstream applications concerning vision, natural language, and to a lesser extent auditory media such as music (those with annotated lyrics and even otherwise; infact, the latter ones could serve as excellent applications for all the modules of the model limned in Figure \ref{fig:1}). As indicated earlier, excellent research has been done in solving visual and linguistic tasks \cite{Sarker2021DeepLA}, especially through modern deep learning approaches \cite{NIPS2012_c399862d, NIPS2017_3f5ee243}. Hence we do not focus on offering novel contributions for this module. As an application, the relatively novel item we consider is ‘Classical Indian Music’ generation and identification, as it involves interacting with multiple modules depicted in Figure \ref{fig:1}, as we hope to convince the reader in the following paragraphs. \item \textbf{Sea of Impressions}: Models storage of both, raw and processed data, but is usually not capable of reasoning or performing complex computations. By raw data, we mean rudimentary data storage, such as in databases or hash tables \cite{6912180}, whereas by processed data we mean data that has been reduced or summarized or distributed by processing directed by the \emph{central intelligence unit}. #### Associative Memories \label{subsubsec:am} Mimicking the processing of electric signals from various sensory input by the brain \cite{STERIADE2005101}, we propose to model such a memory using associative memories, which has been vastly studied \cite{iatropoulos2022kernel, NIPS2016_eaae339c, salvatori2021associative, pham2022generative}. Associative memory models typically comprise of: \begin{enumerate} \item \textcolor{orange}{A memory vault}: this is a structure that actually stores memories, like a neural network \item \textcolor{orange}{An energy function}: governs in some sense the representation of memories stored and the manner in which they stored. \item \textcolor{orange}{Optimization operators}: these operators are defined with respect to the energy function for different tasks such as reading, writing, forgetting, updating, etc. \end{enumerate} Despite ample work in the area, a number of questions remain unanswered. Can an associative memory be designed to retrieve the $k$-nearest memory patterns for a given query pattern? Conventional designs usually focus on the case $k=1$, \cite{salvatori2021associative}. Recent work by \cite{davydov2023retrieving} aims to tackle this problem, but is arguably far from settling this question completely, as it deals only with Hopfield-style networks and does not scale well in its current state. In the same vein we ask, “Can associative memories be used to store variable length patterns, without incorporating artificial padding to make all patterns of equal length?” We would like to avoid adding padding to make the usage of memory more efficient. This could notably have a significant impact on length generalization in Transformers \cite{51525, zhou2024transformers}. \par #### Generalized Energy Based Models \label{subsubsec:gebm} On this note, it is interesting to understand the role of memory in models such as RNNs, MLPs, Transformers, etc. as nicely presented in \cite{deletang2023neural}. However, a number of important questions still remain unanswered, such as theoretically investigating the role of memory in such models, building generative models that have an extendable memory, and adapting such extendable memories to continual learning settings are amongst some of these important ones that deserve attention. Though \cite{yoo2022bayespcn} partially addresses the last question, it does not handle non-i.i.d. settings, and particularly does not work well for queries that are significantly different from the patterns it has memorized. We are interested in designing energy-based models that can continually accommodate patterns as local minima with wide basins of attraction by automatically registering new patterns that are significantly different from previously memorized patterns. This brings us to ask, “Can we design distributed associative memories that store fragments of the patterns instead of the patterns directly, such that inference involves appropriately joining these fragments to produce the queried patterns?” Notice how such a memory would be highly resource-efficient and could enable registering a new pattern only when the fragments that compose it or the order in which they are composed is significantly different from existing patterns. \par</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#### Alignment of Associative Memories \label{subsubsec:align_am}
The work of \cite{salvatori2021associative, yoo2022bayespcn} also motivates a very interesting use case for Predictive Coding Networks, in the broader setup of our PCA model. This is highlighted by the unidirectional arrow from \emph{SI} to the \emph{user}, as this signifies memories being continually suggested to the user, based on the user intent. Now while "intent" is a debatable philosophical concept, we focus on receiving user intent in various forms such language, recordings of focal point tracking \cite{ibrayev2023exploring}, recordings of fMRI signals \cite{brainsci12020228}, etc. The simplest of these to parse is arguably intent received through linguistic media. We elaborate on this in the \emph{AR} module section. 

Another interesting question to ask could be, "Can we design an associative memory that mimicks \emph{semantic memory} \cite{Kumar2020SemanticMA}?" \cite{salvatori2024associative} works in this direction, but doesn't seem to exhibit evidence for multimodal retrieval. Ideally, we would like to be able to assign semantics to memories in a cross-modal fashion. 
The idea of associating associations with human understandable semantics seems to have been overlooked in the literature, to the best of our knowledge. A starting point to address this could be to build associative memories that intertwine with Concept Bottleneck Models \cite{pmlr-v119-koh20a}. Without this, associative memory models seem to simply act as highly compressible, neural, key-value data structures which represent associations via an energy landscape. As indicated in \cite{mordatch2018concept}, the idea of using concepts attached to memories can be linked to distributed memory for compositional concept mappings, discussed earlier. In particular, each fragment could correspond to a particular concept.
We tend to favour energy-based memories as they can closely mimick energy/wave-based representations of data that are typically used in the human brain and do not necessarily require supervised training. Using wave-based representations has recently shown very promising results for augmenting memory in deep learning systems in the exemplary work of \cite{keller2024traveling}. \par 
\subsubsection{The Energy-Memory Duality}\label{subsubsec:em_nexus}
This technique still leaves many open questions, such as designing energy functions that can be described using neural ordinary differential equations \cite{chen2019neural} for memory tasks, theoretical analysis on what kind of wave equations affect memory utilization, so on and so forth. Infact, we eventually aim to advocate for using energy complexity as a metric in measuring efficient learning, and in this particular context, memorization \cite{kıyak2023energy, V24}. However actually deriving one requires significant thought on the theoretical definitions and deductions of memory and energy in the context of memorization, which prompts us to plan this work slightly more down the line. Note that this can highlight an important duality relation between energy and memory, such as Heisenberg's principle in quantum physics \cite{BUSCH_2007}. \par

In an effort to explore alternate ways of making memory utilization efficient, we would like to parallely explore quantum associative memories \cite{ventura1998quantum, Miller_2021}. Prior work has mostly focused on theoretical cases such as binary patterns or reduction of noisy retrieval in NISQ settings. We would like to investigate scaling such systems to more high dimensional and useful data like storage of images or small text phrases, at least as a first step. Note that with increasing the dimension, the amount of noise that can get induced increases exponentially, so this is a challenging problem.

#### Structured Retrieval Enhances Memory Utilization \label{subsubsec: struct_ret_mu}
Infact, this brings us back to our motivation in discussing the importance of the \emph{SI} unit, wherein we consider designing memorization schemes that entail memory stored in so-called unstructured format(s) yet retrieved via 
intelligent retrieval schemes that try to recognize patterns encoded into such unstructured data and retrieve memories. This is opposed to storing data (or predicting outcomes) in structured formats such as Graph Neural Networks \cite{Veli_kovi__2023, veličković2018graph}. A preliminary example of this is the contrast between relational databases and non-relational databases \cite{8284494}. Understandably, this unit closely links to the \emph{Rule Synthesizer} unit, of which a key function is to synthesize rules that enable storage of impressions/sensory input in the \emph{SI} in highly compressed and in an otherwise enigmatic fashion -- indicated by the bi-directional arrows in Figure \ref{fig:1}.

#### Analog Implementation \label{subsubsec:analog}
A $4^{th}$ reason why energy based representations are useful is that they help in building analog neural networks which can be very compute-efficient, as evidenced in the seminal work of \cite{kendall2020training, scellier2023energybased}. Notably, Differentiable Neural Computers (DNCs) \cite{graves2016hybrid} act as neural analogues of modern day computers in their containing an external memory unit alongside a processor. However to the best of our knowledge, there has not been convincingly enough, study, if at all, that investigate implementing DNCs in a manner that can scale well for large scale applications \cite{iscen2024retrievalenhanced}. We therefore, propose to enhance DNCs by exploring them in an analog fashion alongside tweaking their mechanics to learn continually under rewards \cite{abel2023a}. Implementing DNCs this way brings us closer to the \emph{CIU} unit, since in \cite{kendall2020training}, the memory and processing units are sort of merged which makes memory usage very efficient and forces the CIU unit to interact with the SI in an intertwined fashion. Once this is implemented, we would like to borrow ideas of memory management extensively studied in systems literature such as NUMA \cite{10.1145/2508834.2513149}, virtual memory \cite{710872, packer2024memgpt}, etc.\footnote{As such, this line of research complements ML4Systems style works, e.g. \cite{kraska2018case} and others discussed above.} 
Thus this would act as a novel and impactful application of the energy-memory duality thesis proposed in Section \ref{subsubsec:em_nexus} and fits in our broad \textbf{PCA} model. Most importantly, we would expect such a system to produce significantly lesser carbon footprint and reduce energy consumption compared to contemporary large AI models. This is because we aim to explicitly link the energy utilization with the data the model learns to store. In Sections \ref{subsubsec:am} - \ref{subsubsec: struct_ret_mu}, this energy was more of a "virtual" energy, in that it was an abstract mathematical quantity represented on a computer. However, the connotation of energy in this subsection is of its physical consumption, which is comprehensively captured in the framework of analog neural networks \cite{kendall2020training}.

\item \textbf{Scratchpad}: This module primarily acts as a short-term memory module, acting complementary to the \emph{SI} unit that mostly stores impressions compressed and summarized over long episodes. It's necessitated by the fact that certain tasks require only volatile memory, such as numerical or symbolic computations which require registers to store results of intermediate computations. This functionality is represented by means of a unidirectional arrow from the \emph{rule evaluator} unit to the this unit in Figure \ref{fig:1}. In a hierarchical memory context, if the \emph{SI} is treated like a main memory module, the \emph{scratchpad} is akin to a cache \cite{cho2024kvrunahead, dong2024less, pmlr-v202-giannou23a}. 
#### Transformers and Attention Mechanism \label{subsubsec:transformer}
Recently, there's been a surge of interest in exploring such working memory \cite{Miller1956-MILTMN-2} style mechanisms via the attention mechanism and/or Transformer models \cite{darcet2024vision, munkhdalai2024leave, 51142}. We particularly note the significant work of \cite{hwang2024transformerfam} where they incorporate working memory into transformers via introduction of feedback loops, as in RNNs. However, there doesn't seem enough work on exploring the usage of scratchpads in Transformers for performing symbolic computations. This is particularly relevant in a Neurosymbolic context, especially given that the \emph{rule synthesizer} unit can design rules to efficiently store memories in the \emph{SI}. For context-free languages for example, we would expect the \emph{rule synthesizer} unit to synthesize context-free grammar rules which the Transformer should utilize when performing attention. Infact, this also links to a related question if we can implement Transformers that perform "symbolic attention," where the entries from matrices are not necessarily from real numbers but from a finite field. In addition, we would like to explore extensions of the work in \cite{hwang2024transformerfam} by asking, "Is there an equation that connects the capacity of the working memory with the architecture and language complexity of the language the Transformer is learning?" This is relevant, since \cite{hwang2024transformerfam} seemed to observe $64$, as a "sweet spot" on the capacity of the working memory to enable good sequence learning. Linking to a theme discussed earlier, we would also like to explore if we can design an attention mechanism that employs differential equations of the style used in wave-RNNs proposed in \cite{keller2024traveling}.

#### Transients in Counterfactual Simulations \label{subsubsec:tcs}
There is yet another domain of application for this module that connects it to the \emph{Autonoetic Reflector} module, that is primarily responsible for long sequence counterfactual simulations, or counterfactual decision making, so on and so forth. An example that we elaborate later on includes "Counterfactual Interpolation Learning." In these long-sequence counterfactual simulations, we again require computation of intermediate estimates which are solely required for computing the desired quantity and may not have any semantic significance, beyond reuse for future computations of the same nature. We would like to explore usage of this scratchpad that can be jointly used for the kind of discrete computations described earlier as well as those just described above.

#### Theory \label{subsubsec:theory_scratch}
From a theoretical perspective, we would like to explore the usage of a \emph{scratchpad} in terms of its catalytic computational complexity (CCC as we abbreviate it hereafter) \cite{10.1145/2591796.2591874}, which we believe has largely been addressed. This immediately also raises questions such as: "Are there thresholds to CCC in learning languages from the Chomsky hierarchy \cite{deletang2023neural} via the attention mechanism?" "\textcolor{yellow}{Akin to logarithmic complexity circuits, are there techniques to incorporate circuits in Transformer models to}?" "Is there a mechanism or a learning rule we can derive to clearly calibrate the complexity at which data present in the \emph{scratchpad} and selectively transferred to the \emph{SI} in a Transformer?" "How about doing the same in a continual setup, e.g. \cite{9996760}?" These and many other interesting questions remain largely unanswered which prompt us to solve and explore the ramifications in improving such models. 

\item \textbf{Rule synthesizer}: 
The most formal treatment of data compression in terms of learning that we find relevant for this thesis is the framework Universal Artificial Intelligence \cite{deletang2024language, Hutter:07aixigentle}.
#### Algorithmic Information Theory \label{subsubsec:ait}
While there is ample of theoretical work in this domain, scalable applications of the same to be scarce for a number of reasons \cite{10.5555/3020847.3020896}. Most fundamental of them is the fact that many of the standard quantities involved in such frameworks, especially of those arising from Algorithmic Information Theory (AIT) \cite{grunwald2008algorithmic} are either theoretically uncomputable or very hard to compute, such as Chaitin's constant, certain Kolmogorov Complexity related bounds, so on and so forth. In particular, we are interested in designing "rules", that help us reduce the amount of data that needs to be explicitly stored in the \emph{SI}, yet enables us to generate all that needed to be memorized when it is to be retrieved. In other words, memorizing the rule along with some minimal amount of data should enable us to answer queries based on data that we have not explicitly memorized. This tantamounts to the Minimum Description Length (MDL) principle \cite{NEURIPS2018_3b712de4}, for most practical purposes. A simple example is of trying to memorize $1000$s of triplets of numbers of the form $(x, y, x+y)$ vs. learning an abstract representation of the rule of addition itself that saves us from having to memorize these numerous triplets explicitly, yet enabling us to answer with $z = x^{'} + y^{'}$ when given a query $(x^{'},y^{'}, \cdot)$. Again, notice how this is closely linked to the \emph{SI} unit.
Questions we would like to answer with this idea in mind include: "Can we design neural kernels that obey the MDL principle \cite{10.5555/3648699.3648967}?" Very recent work by \cite{hamzi2024bridging} indicates this as a promising question that can be answered with impactful applications. Notice that \cite{hamzi2024bridging} also advocates for tools from AIT to be applicable to a much broader spread of machine learning models. Thus we ask, "Based on the language complexity of a language (e.g. Dyck languages), can we design an attention mechanism that explicitly obeys the \emph{MDL} principle? Or are there existing attention mechanisms such as those specifically developed for inferring infinitely long context \cite{munkhdalai2024leave} that implicitly obey the MDL principle?" \par 
#### General Neural Algorithmic Learners \label{subsubsec:gnal}
However this alone, may not give us the actual algorithms that evolve when intensely compressing data to the limits. In some sense, we need a representation space for algorithms that can be represented by neural networks. Added to this is the fact designing algorithms/model structures based on the MDL alone may not lead to tractable schemes and so would usually involve incorporating other inductive biases as well. This is where we intend to familiarize ourselves with and further the link of work proposed by 'General Neural Algorithmic Learners' \cite{pmlr-v198-ibarz22a}. In light of this, we seek to build neural models that explicitly allow for control flow to be steered into them. \cite{pmlr-v198-ibarz22a} demonstrates Graph Neural Networks as strong contenders for this objective. We hypothesize architectures that accommodate recurrent connections such as RNNs and SSMs (State Space Models) \cite{gu2022efficiently} as also being good candidates for the same task. Thus we ask, "Can we build control-flow-aware neural architectures that implicitly learn algorithms along the weights of the control model?" Notice how this significantly differs from classical algorithms being learnt in \cite{pmlr-v198-ibarz22a}, as in our setup, the algorithms are to be learnt in an unsupervised manner --- it needs to learn the algorithm that compresses the data best, subject to constraints that make such a quantity computable, ofcourse. We would also like to explore how such a solution for SSMs would tackle the exponentially decaying memory problem highlighted in the recent work of \cite{wang2023statespace}. Thus we would like to ask, "Does separating data and control-flow in the way described above help tackle the aforementioned problem by making the SSM to focus on learning the algorithm that computes the data?" We also emphasize how we seek to utilize structured prediction model such as GNNs for learning algorithms than in having them directly operate with data, to keep in line with our underlying thesis that data be stored in unstructured format with retrieval mechanisms being highly structured to make maximum utilization of resources. \par
#### The Differentiable Logic Perspective
However attempting to solve this problem in a purely unsupervised fashion may lead to poor convergence and other optimization issues due to a vast search space (of all possible algorithms!). This inspires us to parallely explore close connections of such methods to neurosymbolic programs. One specific question we can ask here is, "Can we build a neurosymbolic DSL \cite{10.1145/3591280, parisotto2016neurosymbolic} that learns composite rules to compress data based on primitive rules that may or may not be provided in a supervised fashion?" Similarly another very interesting question in this regard is, "Can we build a GAN with a generator that attempts to learn rules that compress the data, and with a discriminator behaving as an agent that attempts to evidence statements that constitute sufficient statistics about the data but cannot be proven based on the rules learnt by the generator so far, as in the constructive proof adopted in G\"odel's Incompleteness Theorem? \cite{Godel1931-GDEOFU, Schmidhuber2007}"

A nice segue from the neurosymbolic thread discussed above involves explicitly involving the logical structure into deep nets, such as in the study of differentiable logic \cite{zimmer2023differentiable, xu2024logicmp, 10.5555/3294771.3294992, NEURIPS2022_0d3496dd,ijcai2022p417}. 
In the context of our thesis on smart data compression we can thus ask some interesting questions as, "Redolent to \cite{10.5555/3618408.3618484} where they build an interpretable concept-based model equipped with fuzzy logic rules, can we build logic structures that compress the data well, given some statistics of the distribution the data comes from?" For example, it may be far more efficient to incorporate a logic network \cite{xu2024logicmp} that implements a neural gate \textsc{AND} than to have a neural network directly. learn a Boolean function of the form $f_{\textsc{AND}}:\{0, 1\} -&gt; \{0, 1\}$. We would also like to understand when logic networks really beat neural networks in learning, and if they do, then in what metrics, and how they relate to the distribution of the underlying data and model. For instance, we could ask, "Do deep nets implicitly learn approximate logic networks?"
"Also, is it possible to build models that can continually adapt their logic structures to have the data\footnote{data stored in the \emph{SI} specifically} adhere to rules obeyed by the network?" Note how this has 2 very interesting connections:
\begin{enumerate}
    \item To neural circuits \cite{MARDER20121}, in computational neuroscience;
    \item To applications to category theory in deep learning \cite{shiebler2021category}, especially with the intuitionistic type theory \href{https://github.com/bgavran/Category_Theory_Machine_Learning}{flavour}.
\end{enumerate}

A salient application of the above ideas lies in improving mathematical reasoning in LLMs \cite{NEURIPS2023_58168e8a, lewkowycz2022solving, NEURIPS2023_44414694, 53097}. Thus we would like to test our ideas based on logic networks, energy-based models, and MDL compression-based approaches elaborated above on increasingly complex mathematical reasoning tasks and see if our networks are able to clearly distinguish (numerical) arguments from proofs that verify the mathematical computations atop those arguments. This would also serve as a good sanity check to move on to incorporating semantics into memories, as discussed earlier in the section on the \emph{SI} module.

#### Episodic Memories \label{subsubsec:episodic}

Next, we would like to tackle the incorporation of long-sequence episodic memories into foundation models as an extension of the ideas for building control-flow-augmented neural processors discussed above \cite{pmlr-v202-karuvally23a, wang2023augmenting, das2024larimar}. This has a close connection with the \emph{SI} module discussed earlier. However from the perspective of the \emph{Rule Synthesizer} we seek to propose representations that inherently support hierarchical generation of memories stacked over long episodes of time, such as via energy functions, \cite{pmlr-v202-karuvally23a} or differential equations \cite{keller2024traveling} or even via geometries such as Poincar\'e embeddings \cite{NIPS2017_59dfa2df}.

#### Is That Right? \label{subsubsec:interp}
Alongside methods of generating rules and incorporating them, it is important to have methods to help us diagnose what rules such models are inherently running. This leads in the exciting direction of interpretability/explainability \cite{bereska2024mechanistic, 8354201}. In contemporary interpretability work, there seems to be no explicit effort to perform mechanistic interpretability over energy-based models and logic networks, to the best of our knowledge. We would like to parallelly address questions in these domains as well. 

#### Theory \label{subsubsec:theory_ciu}
On the theoretical side, we would like to rigorously investigate from a theory of computation \cite{deletang2023neural} perspective, specifically in the context of Transformers \cite{s2024tandem, dehghani2019universal, murty2023pushdown}. Notice that these efforts lead to very interesting results on the capabilities of various models to recognize various formal languages. However, as of now, they seem to lack on $2$ significant fronts:
\begin{enumerate}
    \item There are still a number of open questions, even in the theoretical sense, i.e. "Can state-space models (SSMs) represent all languages in the Chomsky hierarchy?"
    \item Applications of these ideas do not seem to scale well enough. They have also seen limited application in generative modelling \cite{khalighinejad2023approximating}.
    A relatively simple yet presumably insightful question in this regard is the following, "Given tokens from a regular language $\mathcal{L}$, is it possible to design an attention mechanism akin to the indistinguishability criterion of the Myhill-Nerode theorem \cite{cotumaccio2024myhillnerode} to learn the underlying automaton governing the language?" Notice how such an idea could be extended to learning tree automatons as well \cite{JMLR:v16:marusic15a}. Transformers are known to be effective at learning certain "shortcuts" to automata under certain conditions, as proved in \cite{liu2023transformers}, which sets an appropriate precedent for the question(s) we consider here.
\end{enumerate}

Another broad question we would like investigate is on being able to discover provably invariant patterns in data and use them to efficiently solve tasks, using tools from combinatorics (especially the extremal and probabilistic \cite{10.5555/3002498} variants, e.g. the Sunflower lemma \cite{alweiss2021improved} and certain properties of Ramsey numbers, respectively), random graph theory and measure theory (e.g. the simplest one is arguably the Central Limit Theorem in this context!). 
 
\item \textbf{Rule Evaluator}: Abbreviated as the \emph{RE} module, it serves as a comput\emph{er} that computes quantities based on data/rules imbibed from the mnemonic modules and \emph{RS}. 
In this context, we ask questions that concern different model's capacities to compute based on either static logical rules or on dynamically learnt logical rules. Some such questions are as follows: 
\begin{itemize}
    \item The theoretical work of \cite{barcelo2024logical} demonstrates how the slang of attention used in transformers can determine whether it can recognize languages outside a famous circuit complexity class ${\textsc{AC}}^{0}$. This immediately raises $2$ questions: "What is the exact class of languages that\emph{ AHAT}s do recognize?" and of larger interest, "Learning from \cite{10.5555/3618408.3618484}, can we explicitly incorporate logic constraints into Transformers efficiently, such that for an arbitrary $k$, they learn to recognize languages from the circuit complexity class $\textsc{NC}^{k}$?" Notice how this would immediately imply recognizing arbitrary languages from $\textsc{AC}^{k-1}$ by the $\textsc{AC}-\textsc{NC}$ hierarchy \cite{Clote2002BooleanFA}! \textcolor{yellow}{This also promulgates incorporating circuit-based models into computational learning, that are known to recognize languages undecidable for even Turing machines \cite{10.5555/1540612}, which in turn opens many other interesting questions.}

    Investigating such questions in the context of Transformers is particularly relevant as they are capable of learning relational reasoning amongst symbols as shown in \cite{boix2023can}, which strongly relates to the associations that the \emph{SI} module seeks to induce into memories that it absorbs in its representation. We would thus like to investigate building an associative memory complemented Transformer that operates based on the relations that the associative memory stores the memories using, which are in turn computed using possibly other heads of the same Transformer, as discussed earlier in the \emph{RS} unit!
    This could be one of the most compact implementations of the \textbf{PCA} model, barring causal and generative capabilities.
    Therefore this questions seem to be of significant importance, both theoretically and practically.
    
    \item "Can we design PCNs \cite{yoo2022bayespcn} that can learn to encode logical constraints -- first statically and then continually?" PCNs have often been linked to the model of learning in the human brain \cite{choksi2021predify, salvatori2023braininspired}, so incorporating (\emph{first-order}) logic structure into it might serve an interesting use case.
    
    \item Recently KANs \cite{liu2024kan} have garnered lot of attention and interest as powerful alternatives to MLPs. Given that KANs have learnable edge activations, it is natural to expect that with the right control, such networks can perhaps be used for both symbolic tasks and computations based off operations over real vector in continuous space. We would like to investigate in this connection, some potential advantages of KANs in modelling (\emph{first-order}) logical constraints as opposed to MLPs, as has been traditionally done \cite{10.5555/3618408.3618484, parisotto2016neurosymbolic}.
\end{itemize}

\item \textbf{Autonoetic Reflector}: As the name suggests, this module is largely responsible for simulated \textsc{self}-reflection. We delegate most of the causal reasoning related tasks to this module. Thus causal counterfactual generations, decision making, long sequence counterfactual simulations for planning are done in conjunction with the \emph{Rule Synthesizer} unit. This is because we treat Structural Causal Models \cite{poinsot2024learning} as models capable of probabilistic reasoning. Thus, the rules in this context, would be the structural equations to be learnt across the edges, as in Neural Causal Models \cite{xia2022causalneural}. There are a number of questions to be resolved in this domain itself, even ignoring connections to our overall thesis here. 
#### Causal Model Design \label{subsubsec:cmd}
However, in the context of our proposal here, some pertinent questions would be, "Can it incorporate priors defined by the \emph{EI} module to dynamically generate counterfactual samples without changing the architecture significantly, w.r.t the memories in the \emph{SI} module?", "How about doing so over multiple episodes in a causal bandit settings \cite{wagenmaker2023instanceoptimality}?", "Based on counterfactual simulations, can beliefs previously used to store memories in the \emph{SI} module be continually updated?" The last one, is understandably more like an inverse problem. 

#### Model Instantiation \label{subsubsec:mi}
On the implementation side, we can ask, "Do KANs \cite{liu2024kan} provide any benefit in instantiating SCMs as opposed to neural networks?" "Akin to the promising work of \cite{salvatori2024predictive} where they use PCNs, can we demonstrate a compact architecture (possibly PCNs themselves!) that can perform tasks required of the \emph{AR}, \emph{RS} and \emph{SI} modules simultaneously?" Infact, the uni-headed arrow between the \emph{user} and the \emph{SI} module is to possibly design a PCN, that can take user input in various forms and continually propose memories that correlate highly with those inputs. To the best of our knowledge, this question has not been investigated in context of multimodal user inputs and variable length memories. Solving this would have very meaningful applications such as in recommender systems. Beyond correlations, it is possible to enhance user experience by modelling it via an SCM using latent variables to represent user emotions and intent, which is precisely what we would like to investigate more deeply in regards to this \emph{Autonoetic} module \cite{10.1145/3639048}.

An alternate interesting framework that could possibly implement all these operations is Assembly Calculus \cite{doi:10.1073/pnas.2001893117}, which we would also like to explore. More concretely, it is interesting to explore how Optimal Transport between various processes can be implemented using this framework to give arise to different sorts of activity in the proposed brain model.

\item \textbf{Collaboration}: The thick double arrow between the large \textbf{PCA} model and the other (smaller) replicas depicted in Figure \ref{fig:1} symbolize the way these models could collaborate and exchange information -- both in terms of sensory data as well as more abstract data such as algorithms and simulations inferred by the \emph{Rule Synthesizer} and \emph{Autonoetic Reflector} modules respectively. Some initial questions that may be of moderate interest in this direction would be: "Is it possible to communicate latent representations learnt by one \textbf{PCA} model for representing algorithms (in the spirit of \cite{pmlr-v198-ibarz22a}) to another \textbf{PCA} model efficiently?" "How about doing the same in a distributed learning setup?" "Can using multiple \textbf{PCA}s help learn an algorithm from data faster than a single \textbf{PCA} model, especially with finite energy and physical memory?" In particular, "Just like digital communication done through analog waves, can we use wave-based neural ODE \cite{keller2024traveling} techniques to efficiently communicate even simple data such as images/text between these models?" "There are a number of interesting works in multi-agent RL \cite{Zhang2021} that we would like to learn from in this connection.

On the theoretical front, so far there has been limited effort to evaluate learning from a communication complexity perspective \cite{10.1145/800135.804414} \cite{pmlr-v99-kane19a, NIPS2015_7fec306d, 10.1145/1015330.1015351, 4558809}. In this context, we would like to investigate deeper questions such as: "Given the same observational data, what is the minimum communication complexity for a group of \textbf{PCA} models to communicate with each other, to determine the model with the best prior to answer a given set of queries regarding the observations?" Note how this connects to having learned appropriate priors in a Bayesian learning framework \cite{khan2023bayesian}, as touched upon earlier. Another question of interest here is the setup of $2$ \textbf{PCA} models interacting teacher-student (curriculum learning) \cite{8827566}. We ask "Can we design an algorithm where the \emph{teacher} \textbf{PCA} model continually learns the function(s)/rules representing the sensory data based on queries asked by the \emph{student} \textbf{PCA} model, by minimizing the communication complexity between the teacher and student while interacting with the external environment?" Informally, this epitomizes a famous quote attributed to Einstein, "If you can't explain it simply, you don't understand it well enough." The idea is to therefore iteratively improve the understanding of the teacher model, by formulating a mini-max game between the teacher and student models \cite{10.1145/1015330.1015351}. On a related note, we can also ask from a reinforcement learning setup, "Can we design bandit algorithms that minimize the cumulative regret, defined in terms of the communication complexity between $2$ interacting bandits, representing different distributions, in a stochastic environment?"
\item \textbf{Emotional Interpreter}: As described earlier, this module is responsible for tasks such as sentiment analysis and response, sensing emotions intact in other \textbf{PCA} models, so on and so forth, which are truly complex tasks, and which we refrain from delving into further in this proposal. The main skill we would like to model is a rudimentary form of creativity, or more specifically, \emph{generation}.
Thus, this module acts as an interface for generative modeling. As depicted in Figure \ref{fig:1}, this interacts with the \emph{Autonoetic} module, which in turn heavily depends on the \emph{Rule Synthesizer} unit. Thus, this is tasked with generating data according to the rules learned/synthesized by the \emph{Rule Synthesizer} unit and in a Bayesian learning sense, evidenced/conditioned by the impressions absorbed by the \emph{SI} module. There are a number of questions to consider in this regard such as, "Can we design a universal, probabilistic generative model that given joint samples as input can produce conditional, marginal or joint samples as output, given the appropriate query (barring MCMC style approaches, ofcourse!)?", "Can we design energy-based methods to generate samples based on synthesized rules efficiently (analogous to the ideas in \cite{deletang2024language})?", "Utilizing normalizing flows as advocated in \cite{lipman2023flow}, can we induce biases towards choosing appropriate probability paths for transfer learning or cross-skill learning using ideas adopted from BLR \cite{khan2023bayesian}?"

This also gives rise to deeper questions such as "Does there exist a relation in terms of sample complexity/Kolmogorov complexity or otherwise that distinguishes the ability to answer queries about the data vs. being able to generate data from a proxy of the underlying distribution itself?"  \end{itemize}
</code></pre></div></div> <h3 id="miscellaneous-labelsubsecmisc">Miscellaneous \label{subsec:misc}</h3> <p>There are a few topics of independent interest that we wish to pursue that may possibly aid the overall scope of our research concerning \textbf{PCA} models. These mostly concern purely mathematical/theoretical questions, some of which we briefly enlist below: \begin{itemize} \item \textbf{Optimization}: general interest in problems in (non-) convex optimization. One interesting problem in this connection that we have in mind is, “Assume that we have a bounded (non) convex surface (possibly smooth as well). Let us assume that the loss landscape is perfectly reflecting. Given that there is a perfectly reflecting hyperplane that upper bounds the surface, does there exist a specific angle of incidence for any arbitrary point on the landscape against the plane such that the final reflected ray hits a minima precisely normal to the tangent at that point on the surface?” Another fascinating take on optimization, for deep learning specifically is the line of works based off the Bayesian Learning Rule (BLR) \cite{10.5555/3648699.3648980, nickl2023the}. Using \emph{natural} gradients with the appropriate instantiations of the BLR gives arise to solutions to problems from various utilities such as sensitivity \cite{nickl2023the}, model merging \cite{daheim2024model}, etc. We would like to investigate this in the context of energy-based models, which may not have a strict loss function in the sense of conventional supervised learning.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A tangential interest, partly pursued in the main agenda as well, includes ideas from physics-inspired ML \cite{RAISSI2019686, pmlr-v151-wang22a, doumèche2024physicsinformed}. One particular idea we have in mind is to explore adding adiabatic constraints for the general framework proposed in \cite{pmlr-v202-xu23m} for machine unlearning \cite{sekhari2021remember}. A key insight into this is derived from \cite{5392446}. 
\item \textbf{Learning Theory}: Generally interested in studying bounds in various settings in computational learning theory. Notable interests are \cite{JMLR:v22:17-298} because of its general applicability beyond the i.i.d. assumption and \cite{pmlr-v195-mutreja23a} to demonstrate PAC-style bounds in a novel setting of verification (which might be relevant for our \emph{Collaboration} module in Figure \ref{fig:1}!). I would like to explore other utilities as well. We also mull over studying and solving questions in \emph{sample compression} \cite{chase2024dual}, as this strongly relates to our proposed ideas regarding the MDL principle and efficient memory usage, from Figure \ref{fig:1}.  

We are also interested in venturing into inductive biases for SoTA models and algorithms, as this could have a direct impact on our choice of models used for implementing the \textbf{PCA} schema. Specifically, we are currently interesting in exploring inductive biases in the attention mechanisms in the overparametrization regime, which to the best of our knowledge, hasn't been settled \cite{tarzanagh2023maxmargin, tarzanagh2024transformers, vasudeva2024implicit, pmlr-v162-edelman22a, deora2024on, chen2024training, lavie2024understanding}.

We are also very interested in addressing problems in quantum optimization \cite{abbas2023quantum, garg2020quantum, garg2021nearoptimal}. In fact, there is an interesting connection to our main line of work burgeoning on the 'quantum associative memory' idea mentioned earlier in the \emph{SI} module. There is very little work on this, and even existent work is perhaps far away from being practically usable for high-dimensional data such as images and text documents. Ofcourse, there is always the NISQ problem which also motivates us to tackle noisy retrieval and generation problems using variational quantum algorithms \cite{cerezo2021variational} as well as adiabatic quantum computing \cite{farhi2014quantum}. An open problem in this direction is, "Does there exist an algorithm for generating conditional samples given joint samples of some distribution with say, $\textsc{poly}(\textsc{log}(N))$ qubits, where $N$ is the number of joint samples provided?" Lastly, but not least, this could have a phenomenal impact for foundation models, which we would like to explore towards the end of this work. Briefly speaking, (V/L)LMs have been tremendously successful in generating responses given user-driven queries. "Can we build a Quantum Foundation Model (QFM) that using much fewer qubits can store long-sequence data that can then be used by standard probabilistic inference algorithms to respond to such queries?" A key challenge here would be to figure out how to continually refurnish qubits on which measurement operators have already been operated upon so that they do not lose the knowledge encoded into them. We believe that these problems and other such research interests would have a significant contribution to the goals of industry-level AI \cite{Liu_2024}. 

\item \textbf{Discrete Math}: Generally interested in theoretical computer science with specific interests in combinatorics \&amp; graph theory alongside computational complexity. With respect to the former, extremal combinatorics, especially for unearthing natural patterns in large amounts of data, e.g. the Sunflower Lemma \cite{alweiss2021improved}, applications of the probabilistic method, polynomial methods, etc. are of keen interest. In fact, we are still looking forward to fully settling the open problem raised in \cite{balachandran2018fractional}. 
From the computational complexity angle, alongside the pristine questions in the same, we would specifically like to investigate applications of the PCP theorem \cite{cryptoeprint:2021/915} in an interactive machine learning setup \cite{goldwasser_et_al:LIPIcs.ITCS.2021.41}. Notably, this is not straightforward since the proof for the verifier in the standard PCP theorem seems combinatorially quite random \cite{10.5555/1540612} and thus may not find plug-and-play use in machine learning. \end{itemize}
</code></pre></div></div> <h2 id="candidates-pedigree-labelseccp">Candidate’s Pedigree \label{sec:CP}</h2> <p>The rigorous logic and systematic structure present in the study of discrete objects in mathematics always impressed me, as opposed to rote memorization and blind usage of derived formulae, even as a growing adolescent. This interest to view mathematical creativity grew in me and even in middle school, I already found myself trying to solve Olympiad level problems out of that interest. During this period, I was privileged to have got selected for the \emph{Indian National Mathematical Olympiad}. Fast-forwarding a few years, at IIT-H, I got introduced to Computer Science \&amp; Engineering (CSE) which opened me up to a much broader landscape of exciting problem solving. I learnt to use known mathematical objects/frameworks and then amend/utilize them to setup designs and algorithms for solving real-life, everyday problems. Furthermore, as budding engineers we were trained in best practices to practically deploy such designs in systems to solve real-life problems – this was the most interesting and enthralling aspect of the study and research that I undertook in CSE. \par Through my undergraduate study in CSE, I was fortunate to have been able to maintain a healthy balance between Theory and Practice. It started with learning CSE fundamentals beginning from coding and basic algorithms to understanding advanced systems such as wireless 5G networks, Reinforcement/Deep Learning based applications in computer vision, games, auctions and Quantum Computing. In my pre-final year, I was privileged to have interned at Microsoft, India where I worked in the R\&amp;D team for Edge (Anaheim). In particular I worked on features such as OCR for scanned PDFs in order to improve their accessibility, and got used to writing industry level code. I also got myself acquainted with certain popular ML-based models for OCR, which gave me a taste of applied ML in industries. I was fortunate to also be involved in designing a mini-architecture for integrating the proposed feature into the browser code. The code I developed was in a state almost ready for an official feature roll out and the Microsoft team was pleased to offer me a PPO for my work. \par My interest in theoretical computer science and math enthused me to explore research side by side and get started early on with it. My first project was advised by \href{https://people.iith.ac.in/rogers/}{Prof. Rogers Mathew}, where I worked on improving existing bounds on the size of certain classes of extremal families, known as Hierarchically Closed Fractional Intersecting families. This problem has an interesting connection to Matrix Theory \cite{balachandran2024low}. Starting from ground zero, we built up a sequence of simple, logical and creative arguments which helped get a linear bound on the size of the aforementioned class of families, a novel result indeed. Collaboration helped bring rigor and a couple of other interesting insights to our work. This resulted in a journal paper published at the reputed Electronic Journal of Combinatorics \cite{balachandran2022hierarchically}, which was viewed very positively by reviewers, as it provided a breakthrough for a particular case-cum-generalization of a problem open for $5$ odd years \cite{balachandran2018fractional}! Motivated by this positive outcome in rather a short duration of time helped me overcome my inhibitions about the stochasticity of research in general. \par I also dabbled with Quantum Computing in working on project supervised by \href{https://www.iith.ac.in/cse/mvp/}{Prof. M.V. Panduranga Rao}, where we developed a PoC to detect bug fix patterns in Quantum software, analogous to efforts in classical software for the same, \cite{Kher23}, \cite{nayak2023qpac}. This along with a mini-project concerning software development for error correction in quantum communications greatly enhanced my interest in the field of Quantum computing. \par By now I was clear to pursue a Ph.D, and abjured the PPO from Microsoft and an offer from a (stealth-mode) quantum technology startup to pursue the same, at IIT-H itself, owing to the excellent research ecosystem here. I am fortunate to be advised by \href{https://people.iith.ac.in/vineethnb/}{Prof. Vineeth N. Balasubramanian} and \href{https://people.iith.ac.in/saketha/}{Prof. Sakethanath Jagarlapudi}. Fast-forwarding a bit, the research projects I am currently involved with include the following: \begin{enumerate} \item \textbf{Causal Generative Modelling with Multimodal LLM Feedback}: Most causal generative models require annotations/labels over the training data to either explicitly or implicitly learn an underlying causal graph, using which new samples can be generated, e.g. CausalVAE/CausalGAN \cite{yang2023causalvae, kocaoglu2018causalgan}. This is clearly not always possible, such as due to dearth of enough human resources, or of ethical concerns in collecting certain specific types of data, etc. While in general, it’s perhaps impossible to replace properly labelled data with any other source of supervision, we wish to explore LLMs as sources of supervision, at least for some common/well-known datasets, with the hope of that they augment the data well enough. Inheriting a lot of the setting from CausalVAE, we focus on image datasets. This prompts us to use VLMs such as GPT-4, CLIP, etc. as rich reserves of semantics for concepts that “causally define” such images. However, to actually get the annotations for each of these causal concepts, we require a module that can generate real values corresponding to each of these causal concepts. Label-free CBMs \cite{oikarinen2023labelfree} serve as a viable solution to this problem. Our methodology thus involves smartly convolving the pipelines of these 2 works to get an end2end automated CausalVAE unit. In the overall picture, it seeks to study the relation between \emph{energy transducers} and the \emph{autonoetic module}. \item \textbf{Leveraging Optimal Transport for Generative Causal Inference}: Comes under the general idea of using OT as a useful tool to solve problems in probabilistic inference. Here the task is to adhere to the strict notion of counterfactuals as per Pearlian causality and generate true counterfactuals, given observed data alone, and the assumption that exogenous sources of noise for the source and target variables are independent. Many works seem to only loosely address the counterfactual generation problem. For instance, in the context of image generation, not all image-editing based tasks can truly be considered ‘counterfactual image generation.’ We thus match the joint distribution being integrated over in standard OT \cite{pmlr-v238-manupriya24a}, with a joint distribution of observables and counterfactuals. We also extend this idea to a novel setting of translation in a separate causal space. This work fits in between the \emph{emotional interpreter} and \emph{ autonetic module}. \item \textbf{Search in Energy Space via AMs}: Associative memories (AMs) are typically viewed as content-based retrieval systems: given an input key that bears some form of association with a value, they return the value of interest. Such systems are generally equipped with An energy function; An underlying memory structure/model; Optimization operators, that operate on the energy function based on some downstream utility, such as reading, writing, or forgetting. Models such as MCHN \cite{ramsauer2021hopfield}, BayesPCN \cite{yoo2022bayespcn}, illustrate the practical utility of such a system on accurate image retrieval, given noisy or partial versions of the desired image. However, existing literature seems to lack an AM model capable of performing a $k$-NN style retrieval, i.e. to retrieve the $k$ most similar images from the memory, give an input image. Notice that this is significantly different from vanilla $k$-NN since the training data is no more explicitly available as opposed to the standard case. All the memories are now absorbed into the weights of the model, and thus our task can be dubbed as performing ’search in an energy space’. We therefore propose an optimization-based approach, that tries to sample local minima close around, but not equal to the local minima the original query vector is intended to converge to. For $k=1$, this translates to finding the closest local minima to a given local minima, not equal to it. We expect to solve this using Proximal Gradient Descent. This ties in with the idea of modelling \emph{Sea of Impressions} as an associative memory. \item \textbf{Inductive Bias of Attention Mechanism} Studying the aforementioned in the overparametrization regime, building upon analysis of previous works such as \cite{tarzanagh2023maxmargin}. \end{enumerate}</p> </div> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Krishn V. Kher. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"dropdown-publications",title:"Publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"Blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"A growing collection of project pages, open source softwares, libraries and the like. To be updated SOON!",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-research-statement",title:"Research Statement",description:"An exhaustive list of research directions of particular interest",section:"Posts",handler:()=>{window.location.href="/blog/2024/research-statement/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-served-as-a-sub-reviewer-for-icml-39-24",title:"Served as a (sub-) reviewer for ICML&#39;24.",description:"",section:"News"},{id:"news-finished-ph-d-coursework",title:"Finished Ph. D coursework!",description:"",section:"News"},{id:"news-serving-as-a-sub-reviewer-at-neurips-39-24",title:"Serving as a (sub-) reviewer at NeurIPS&#39;24.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%65%74%61%6B%72%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/https://orcid.org/0009-0000-9391-292X","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=590VrrkAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/KrishnKher","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/355/8033.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>